# ğŸ¦ Pegadaian ETL Data Dummy Pipeline

![Python](https://img.shields.io/badge/python-v3.9+-blue.svg)
![Apache Airflow](https://img.shields.io/badge/Apache%20Airflow-2.0+-orange.svg)
![PySpark](https://img.shields.io/badge/PySpark-3.0+-yellow.svg)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-13+-blue.svg)

An end-to-end ETL (Extract, Transform, Load) data pipeline built for Pegadaian (Indonesian pawn shop chain) sales data processing using modern data engineering tools and medallion architecture.

## ğŸ“Š Project Overview

This project implements a robust, scalable ETL pipeline that processes sales transaction data through multiple layers of transformation, following the **Medallion Architecture** (Bronze-Silver-Gold) pattern for optimal data quality and governance.

### ğŸ¯ Business Problem
- Manual data processing was time-consuming and error-prone
- Need for automated daily sales reporting
- Requirement for data quality validation and monitoring
- Scalable solution for growing data volumes

### ğŸ’¡ Solution
Automated ETL pipeline with workflow orchestration, data quality checks, and real-time monitoring capabilities.

## ğŸ—ï¸ Architecture

```
ğŸ“ Raw CSV Data
    â†“
ğŸ¥ˆ Silver Layer (Data Ingestion)
    â†“ 
ğŸ¥‰ Bronze Layer (Data Cleaning & Transformation)
    â†“
ğŸ¥‡ Gold Layer (Business-Ready Data)
    â†“
ğŸ—„ï¸ PostgreSQL Data Warehouse
```

### Data Flow:
1. **Silver Layer**: Raw CSV files â†’ Clean Parquet format
2. **Bronze Layer**: Data standardization, null handling, calculated fields
3. **Gold Layer**: Final enrichment with timestamps â†’ PostgreSQL

## ğŸ› ï¸ Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Orchestration** | Apache Airflow | Workflow management & scheduling |
| **Processing** | PySpark | Distributed data processing |
| **Database** | PostgreSQL | Data warehouse |
| **Language** | Python 3.9+ | ETL scripting |
| **Storage** | Parquet | Intermediate data storage |

## ğŸ“‚ Project Structure

```
pegadaian/
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .gitignore                   # Git ignore rules
â”œâ”€â”€ 
â”œâ”€â”€ airflow/                     # Airflow configuration
â”‚   â”œâ”€â”€ airflow.cfg             # Airflow settings
â”‚   â”œâ”€â”€ dags/                   # DAG definitions
â”‚   â”‚   â””â”€â”€ etl_penjualan_dag.py # Main ETL DAG
â”‚   â””â”€â”€ logs/                   # Airflow logs
â”‚
â”œâ”€â”€ data/                       # Data files
â”‚   â””â”€â”€ penjualan_2025-07-03.csv # Sample sales data
â”‚
â”œâ”€â”€ etl_scripts/                # ETL processing scripts
â”‚   â”œâ”€â”€ silver_etl.py          # Silver layer processing
â”‚   â”œâ”€â”€ bronze_etl.py          # Bronze layer processing
â”‚   â””â”€â”€ gold_etl.py            # Gold layer processing
â”‚
â”œâ”€â”€ config/                     # Configuration files
â”‚   â””â”€â”€ postgresql-42.2.20.jar # PostgreSQL JDBC driver
â”‚
â””â”€â”€ output/                     # Generated output files
    â”œâ”€â”€ silver_output.parquet/
    â””â”€â”€ bronze_output.parquet/
```

## âš¡ Features

- âœ… **Automated Daily Processing**: Scheduled runs via Airflow
- âœ… **Data Quality Validation**: Built-in checks and error handling
- âœ… **Fault Tolerance**: Retry mechanisms and failure notifications
- âœ… **Monitoring & Logging**: Real-time pipeline monitoring
- âœ… **Scalable Architecture**: Medallion pattern for data governance
- âœ… **Database Integration**: Direct PostgreSQL integration
- âœ… **Modular Design**: Separate ETL scripts for maintainability

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- Apache Airflow 2.0+
- PostgreSQL 13+
- Java 8+ (for Spark)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/pegadaian-etl-pipeline.git
cd pegadaian-etl-pipeline
```

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Set up PostgreSQL database**
```sql
CREATE DATABASE etl_pegadaian;
CREATE SCHEMA gold;
```

4. **Configure Airflow**
```bash
export AIRFLOW_HOME=$(pwd)/airflow
airflow db init
```

5. **Start the pipeline**
```bash
# Start Airflow scheduler
airflow scheduler &

# Start Airflow webserver
airflow webserver --port 8080 &
```

6. **Access Airflow UI**
Open http://localhost:8080 in your browser

## ğŸ”§ Configuration

### Database Settings
Update the connection parameters in the DAG file:
```python
conn = psycopg2.connect(
    host="localhost",
    port=5432,
    database="etl_pegadaian",
    user="your_username",
    password="your_password"
)
```

### File Paths
Ensure the JDBC driver path is correct in `gold_etl.py`:
```python
.config("spark.jars", "/path/to/postgresql-42.2.20.jar")
```

## ğŸ“ˆ Pipeline Stages

### 1. Silver Layer (`silver_etl.py`)
- **Input**: Raw CSV files
- **Process**: Basic data ingestion and format conversion
- **Output**: Clean Parquet files
- **Transformations**: 
  - CSV to Parquet conversion
  - Schema validation
  - Basic data type casting

### 2. Bronze Layer (`bronze_etl.py`)
- **Input**: Silver Parquet files
- **Process**: Data cleaning and standardization
- **Output**: Processed Parquet files
- **Transformations**:
  - Null value handling (replace with 0)
  - Product code standardization (uppercase)
  - Total price calculation
  - Data validation

### 3. Gold Layer (`gold_etl.py`)
- **Input**: Bronze Parquet files
- **Process**: Final enrichment and loading
- **Output**: PostgreSQL `gold.penjualan` table
- **Transformations**:
  - Timestamp addition
  - Column reordering
  - Database insertion

## ğŸ“Š Sample Data

The pipeline processes sales transaction data with the following schema:

| Column | Type | Description |
|--------|------|-------------|
| `id_transaksi` | String | Transaction ID |
| `id_produk` | String | Product ID |
| `jumlah` | Integer | Quantity |
| `harga_satuan` | Decimal | Unit price |
| `total_harga` | Decimal | Total amount |
| `tanggal_transaksi` | Date | Transaction date |
| `inserted_at` | Timestamp | ETL processing time |

## ğŸ” Monitoring & Troubleshooting

### Airflow Web UI
- **DAG Overview**: http://localhost:8080
- **Task Logs**: Available in Airflow UI under each task
- **Pipeline Status**: Real-time monitoring of task execution

### Common Issues

1. **Script not found error**
   - Check file paths in the DAG
   - Verify AIRFLOW_HOME is set correctly

2. **Database connection issues**
   - Verify PostgreSQL is running
   - Check connection parameters
   - Ensure database and schema exist

3. **Spark/Java issues**
   - Verify Java installation
   - Check JDBC driver path
   - Ensure sufficient memory allocation

## ğŸ“ Development

### Adding New Transformations
1. Create new ETL script in the project root
2. Add task to the DAG in `etl_penjualan_dag.py`
3. Update task dependencies as needed

### Testing
```bash
# Test individual ETL scripts
python3 silver_etl.py
python3 bronze_etl.py
python3 gold_etl.py

# Test full DAG
airflow dags test etl_penjualan_dag 2025-07-08
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¨â€ğŸ’» Author

**Raditya Aruan**
- LinkedIn: radityaaruan
- GitHub: @radityaaruan
- Email: radityaruan@gmail.com

## ğŸ™ Acknowledgments

- Apache Airflow community for excellent documentation
- PySpark team for powerful data processing capabilities
- PostgreSQL community for robust database solution

---

â­ If you found this project helpful, please give it a star!
